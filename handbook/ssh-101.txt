1. First, generate a public/private key for your machine (client):
$ ssh-keygen -t ed25519 -C "100067942@ku.ac.ae"
Public key created in `~/.ssh/id_ed25519.pub`

2. Then, provide the remote machine (server) your public key (to establish secure connection):
Add your public key to `~/.ssh/authorized_keys` on the server

3. Now you can remotely connect to the server:
$ ssh -p <port_#> <your_user>@<host_name>  # e.g. ssh 100067942@gpu.lab.ku
Then enter your password when prompted (password giving by admin)

If you don't want to type your password on every ssh login:
Create a config file in `~/.ssh/config`:
```
Host lab_gpu
  HostName gpu.lab.ku
  User 100067942
  Port 22
  IdentityFile ~/.ssh/id_ed25519
```
Then you can ssh directly: $ ssh lab_gpu

4. Move files across machines:
$ scp file.txt lab_gpu:~/   # copy local → remote
$ scp lab_gpu:~/file.txt .  # copy remote → local

5. You can run one-off commands via:
$ ssh lab_gpu "nvidia-smi"


Useful commands:
- See GPUs and driver/toolkit:
$ nvidia-smi

- Run your code:
$ CUDA_VISIBLE_DEVICES=0 python train.py --cfg configs/exp.yaml

- If scheduler is used (SLURM):
```zsh
# request 1 GPU for 2 hours with 16GB RAM
salloc --gres=gpu:1 --time=02:00:00 --mem=16G
# (now you’re on a compute node)
module load cuda/12.4  # if the cluster uses environment modules
conda activate ml
python train.py
```

- Using tmux:
tmux new -s train
# start your training...
# detach:
Ctrl-b d
# later:
tmux attach -t train
